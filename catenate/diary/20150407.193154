nb compress write source code develop

Muratori, C. 20140518.  Working on The Witness, part 11:
Semantic compression.
web http://mollyrocket.com/casey/stream_0019.html

So what does compression-oriented programming look like, and why is it
efficient?  Like a good compressor, I don’t reuse anything until I
have at least two instances of it occurring.  Many programmers don’t
understand how important this is, and try to write “reusable” code
right off the bat, but that is probably one of the biggest mistakes
you can make.  My mantra is, “make your code usable before you try to
make it reusable”. 
	I always begin by just typing out exactly what I want to happen
in each specific case, without any regard to “correctness” or
“abstraction” or any other buzzword, and I get that working.  Then,
when I find myself doing the same thing a second time somewhere else,
that is when I pull out the reusable portion and share it, effectively
“compressing” the code.  I like “compress” better as an analogy,
because it means something useful, as opposed to the often-used
“abstracting”, which doesn’t really imply anything useful.  Who cares
if code is abstract?
	Waiting until there are (at least) two examples of a piece of
code means I not only save time thinking about how to reuse it until I
know I really need to, but it also means I always have at least two
different real examples of what the code has to do before I try to
make it reusable.  This is crucial for efficiency, because if you only
have one example, or worse, no examples (in the case of code written
preemptively), then you are very likely to make mistakes in the way
you write it and end up with code that isn’t conveniently reusable.
This leads to even more wasted time once you go to use it, because
either it will be cumbersome, or you will have to redo it to make it
work the way you need it to.  So I try very hard to never make code
“prematurely reusable”, to evoke Knuth.
	Similarly, like a magical globally optimizing compressor
(which sadly PKZip isn’t), when you are presented with new places
where a previously reused piece of code could be reused again, you
make a decision: if the reusable code is already suitable, you just
use it, but if it’s not, you decide whether or not you should modify
how it works, or whether you should introduce a new layer on top of or
underneath it.  Multiresolution entry points are a big part of making
code resuable, but I’ll save discussion of that for a later article,
since it’s a topic unto itself.
	Finally, the underlying assumption in all of this is, if you compress
your code to a nice compact form, it is easy to read, because there’s
a minimal amount of it, and the semantics tend to mirror the real
“language” of the problem, because like a real language, those things
that are expressed most often are given their own names and are used
consistently.  Well-compressed code is also easy to maintain, because
all the places in the code that are doing identical things all go
through the same paths, but code that is unique is not needlessly
complicated or separated from its use.  Finally, well-compressed code
is easy to extend, because producing more code that does similar
operations is simple, as all the necessary code is there in a nicely
recomposable way.
	These are all things that most programming methodologies
claim to do in an abstract fashion (build UML diagrams, make class
hierarchies, make systems of objects, etc.), but always fail to
achieve, because the hard part of code is getting the details right.
Starting from a place where the details don’t exist inevitably means
you will forget or overlook something that will cause your plans to
fail or lead to suboptimal results.  Starting with the details and
repeatedly compressing to arrive at the eventual architecture avoids
all the pitfalls of trying to conceive the architecture ahead of time.


nb cbabbage garbage in out wrong figures right answers
web 'http://en.wikipedia.org/wiki/Garbage_in,_garbage_out'

Early programmers were required to test virtually each program step
and cautioned not to expect that the resulting program would "do the
right thing" when given imperfect input.  The underlying principle was
noted by the inventor of the first programmable computing device
design:
	On two occasions I have been asked, "Pray, Mr. Babbage, if you
put into the machine wrong figures, will the right answers come out?"
…  I am not able rightly to apprehend the kind of confusion of ideas
that could provoke such a question.—Charles Babbage, Passages from the
Life of a Philosopher
